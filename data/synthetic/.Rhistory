idx=2*j-1
col1=size*idx+1
row1=size*(idx-1)+1
clusInit=col1-size
}
col2=col1+size-1
row2=row1+size-1
clusEnd=col2
corrMat[row1:row2,col1:col2]=M
corrMat[col1:col2,row1:row2]=M
partMat[k,clusInit:clusEnd,i]
}
}
k
corrMat=matrix(NA,N,N)
partMat=matrix(NA,N,(levels+1))
for(i in 1:(levels+1)){
size=N/Nclus[i]
if(i == 1){ # the first level has as many block distances as clusters
Nblocks=Nclus[i]
}else{ # and half off diagonal. The remainder clusters only have off diagonal blocks
Nblocks=Nclus[i]/2
}
k=0
for(j in 1:Nblocks){
k=k+1 # this index will create a factor variable to identify the clusters
M=matrix(rnorm(size*size,mean=breaks[i],sd=stdv),size,size)
if(i == 1){ # The first level fills diagonal blocks
idx=j-1
col1=size*idx+1
row1=col1
clusInit=col1
}else{ # all remaining blocks are off diagonal
idx=2*j-1
col1=size*idx+1
row1=size*(idx-1)+1
clusInit=col1-size
}
col2=col1+size-1
row2=row1+size-1
clusEnd=col2
corrMat[row1:row2,col1:col2]=M
corrMat[col1:col2,row1:row2]=M
partMat[clusInit:clusEnd,i]=k
}
}
View(partMat)
Nperm0=99
setMethod="anosim"
for(i in 1:levels){ # for each level
if(setMethod=="anosim"){
AA=anosim(DistCor.dist, as.factor(partMat[,i]), permutations = Nperm0)
anosimSumm[i]=AA$statistic
anosimSig[i]=AA$signif
}else{
tmp.frame=as.data.frame(partMat[,i])
colnames(tmp.frame)="X"
AA=adonis2(DistCor.dist~X , tmp.frame, permutations = Nperm0)
anosimSumm[i]=AA$F[1]
anosimSig[i]=AA$`Pr(>F)`[1]
}
}
libary(vegan)
library(vegan)
for(i in 1:levels){ # for each level
if(setMethod=="anosim"){
AA=anosim(DistCor.dist, as.factor(partMat[,i]), permutations = Nperm0)
anosimSumm[i]=AA$statistic
anosimSig[i]=AA$signif
}else{
tmp.frame=as.data.frame(partMat[,i])
colnames(tmp.frame)="X"
AA=adonis2(DistCor.dist~X , tmp.frame, permutations = Nperm0)
anosimSumm[i]=AA$F[1]
anosimSig[i]=AA$`Pr(>F)`[1]
}
}
DistCor=sqrt(2*(1-corrMat))
as.dist(DistCor)-> DistCor.dist
attr(DistCor.dist, "method") <- "dist"
anosimSumm=matrix(NA,nrow=(length(distcuts)-1),ncol=1)
anosimSig=matrix(NA,nrow=(length(distcuts)-1),ncol=1) # Will not be printed, since there is plotCtrl below for that
anosimSumm=matrix(NA,nrow=levels,ncol=1)
anosimSig=matrix(NA,nrow=levels,ncol=1) # Will not be printed, since there is plotCtrl below for that
Nperm0=99
setMethod="anosim"
for(i in 1:levels){ # for each level
if(setMethod=="anosim"){
AA=anosim(DistCor.dist, as.factor(partMat[,i]), permutations = Nperm0)
anosimSumm[i]=AA$statistic
anosimSig[i]=AA$signif
}else{
tmp.frame=as.data.frame(partMat[,i])
colnames(tmp.frame)="X"
AA=adonis2(DistCor.dist~X , tmp.frame, permutations = Nperm0)
anosimSumm[i]=AA$F[1]
anosimSig[i]=AA$`Pr(>F)`[1]
}
}
plot(anosimSumm)
dev.off()
dev.off()
plot(anosimSumm)
library(vegan)  # For Mantel test and anosim
library(stringi)
library(dendextend) # for dendrograms
library(permute)
library(ggplot2)
library(ade4)
library(matrixStats) # for row/column stdv
library(lattice) # to combine plots in the same...
library(gridExtra) # ... window with grid.arrange
library(reshape2)
## START EDITING
# Randomization parameters
Nperm0=999 # Number of permutations for the significance of the observed data
Nperm1=1 # Number of permutations for the significance of the (already permuted) data. It may be interesting
# to use a large number to see if a permuted dataset is still significant. Otherwise, since we aim to test the
# significance of the observed data it is not retrieved, and can be low for computational efficiency.
# Fix some output labels and directories ------------------------------------
plotCtrl=0 # If =1 it will plot the permuted distro for each anosim analysis
distLabel="SparCC" # SparCC or SJD
setLabel="Time0" # Do not edit this
rm(list=ls())
library(vegan)  # For Mantel test and anosim
library(gplots)
library(ggplot2)
library(matrixStats) # for row/column stdv
library(reshape2)
library(gtools) # odd function
# Setting parameters and input/output files ----------------------
# --- Parameters to create the random matrix
levels=8 # Number of levels in the interval [-1,1]. One gaussian distribution will be generated around each level
stdv=1 # Standard deviation
plotDist=0 # Should I create a heatmap with the generated matrix (=1)?
# --- Parameters for the anosim/adonis methods
Nperm0=99 # Number of permutations for the significance of the observed data
Nrnd=50 # Number of shuffled realizations for each nested level
Nperm1=1 # Number of permutations for the significance of the (already permuted) data. It may be interesting
# --- Output labels and directories
#plotCtrl=1 # If =1 it will plot the permuted distro for each anosim analysis
distLabel="DistRand" # DistRand, since we are usingsynthetic distances
setLabel=paste("std",stdv,sep="") # Do not edit this
setMethod="adonis2" # anosim, mrpp or adonis2
setStructure="intermittent" # linear, intermittent, random, inverse, meanVar, stdvInverse
# Preliminaries ----------------
# --- Build labels and set working directory
if(setStructure=="meanVar"){
stdv="increasing"
}else if(setStructure=="stdvInverse"){
stdv="decreasing"
}
# ... build labels
labelOut=paste(distLabel,setLabel,setMethod,setStructure,sep="_")
dirOut="/home/apascual/Nextcloud/Research/Projects/FunctionalGroups/DistanceDecay/answer2Refs/randomDistances"
# The following would be used for the hierarchical rand procedure
fileMeanOut=paste("DistanceDecay_HierarchRandMean_",labelOut,".dat",sep="")
fileStdOut=paste("DistanceDecay_HierarchRandStdErr_",labelOut,".dat",sep="")
fileTableOut=paste("DistanceDecay_ObsVsRandAndStdErr_",labelOut,".dat",sep="")
setwd(dirOut)
# Create a random matrix ------------------
# --- Global values of the matrix
Nclus=2^seq(from=levels,to=1) # Number of clusters per level
Nclus=c(Nclus[1],Nclus) # the first level should be repeated to generate diagonal and off diagonal blocks
N=Nclus[1]*4 # total number of elements, four in each cluster in the finer classification
Nelem=N/Nclus # number of elements per cluster in each classification
# --- Fix the breaks along the interval, corresponding to the means of the gaussian distro
mean_int=2/(levels+1) # 2 corresponds to the size of the interval -1:1, where we will build our metric
stdv_int=mean_int/2
means=matrix(NA,1,(levels+1)) # this vector will contain the means of each level
stdv_vec=matrix(NA,1,(levels+1))
means[1]=1-mean_int/2 # from most similar
if((setStructure=="meanVar")||(setStructure == "stdvInverse")){
stdv_vec[1]=stdv_int
}else{
stdv_vec[1]=stdv
}
# --- Create the basic structure (linear model)
for(i in 2:(levels+1)){ # the breaks of the interval correspond to the mean of the correlations
means[i]=means[i-1]-mean_int # decrease the similarity
if((setStructure=="meanVar")||(setStructure == "stdvInverse")){
stdv_vec[i]=stdv_vec[i-1]+stdv_int # increase the stdv constantly
}else{
stdv_vec[i]=stdv # keep it fixed
}
}
# --- Modify it if other model is considered
if(setStructure=="intermittent"){ # every two levels come back to a high similarity
for(i in 2:(levels+1)){ # the breaks of the interval correspond to the mean of the correlations
if((odd(i)==TRUE)&& i != (levels+1)){
means[i]=means[1]
}
}
}else if(setStructure == "random"){ # Randomize linear
#setRand=sample.int((levels+1),(levels+1), replace = TRUE) # result 9 1 1 9 2 6 6 5 5
setRand=c(9,1,1,9,2,6,6,5,5) # fixed for reproducibility
means=means[setRand]
}else if(setStructure == "inverse"){ # invert the order
idx=seq(from=(levels+1),to=1)
means=means[idx]
}else if(setStructure == "stdvInverse"){ # invert for the stdv
idx=seq(from=(levels+1),to=1)
stdv_vec=stdv_vec[idx]
}
means # double ckeck
stdv_vec
# --- Build the correlation matrix
corrMat=matrix(NA,N,N) # Initialize correlation matrix
partMat=matrix(NA,N,(levels+1)) # Create a matrix of factors identifying the identity of the clusters
corrMatMean=matrix(0,(levels+1))
for(i in 1:(levels+1)){
size=N/Nclus[i]
if(i == 1){ # the first level has as many block distances as clusters
Nblocks=Nclus[i]
}else{ # and half off diagonal. The remainder clusters only have off diagonal blocks
Nblocks=Nclus[i]/2
}
k=0
Ncells=size*size
for(j in 1:Nblocks){
k=k+1 # this index will create a factor variable to identify the clusters
M=matrix(rnorm(Ncells,mean=means[i],sd=stdv_vec[i]),size,size)
M[which(M< -1)]=-1 # This can generate heavily U-shaped distros for high stdv
M[which(M>1)]=1
if(i == 1){ # The first level fills diagonal blocks
idx=j-1
col1=size*idx+1
row1=col1
clusInit=col1
}else{ # all remaining blocks are off diagonal
idx=2*j-1
col1=size*idx+1
row1=size*(idx-1)+1
clusInit=col1-size
}
col2=col1+size-1
row2=row1+size-1
clusEnd=col2
corrMat[row1:row2,col1:col2]=M
corrMat[col1:col2,row1:row2]=M
partMat[clusInit:clusEnd,i]=k
corrMatMean[i]=corrMatMean[i]+sum(corrMat[clusInit:clusEnd,clusInit:clusEnd])
}
if(i==1){
norm=Ncells*Nblocks
}else{
norm=4*Ncells*Nblocks
}
corrMatMean[i]=corrMatMean[i]/norm
}
max(corrMat)  # check that the max is not larger than 1
min(corrMat) # check that the min is not lower than -1
DistCor=sqrt(2*(1-corrMat)) # Transform into distance
DistMeans=sqrt(2*(1-corrMatMean))
as.dist(DistCor)-> DistCor.dist
attr(DistCor.dist, "method") <- "dist"
# --- Plot the distance matrix
if(plotDist==1){
source("/home/apascual/Dropbox/alberto.pascual/Dropbox/Research/Programs/libraries/R/heatmap.2.mod.R")
figureOut=paste("heatmap_",labelOut,".pdf",sep="")
pdf(figureOut,width=20,height = 20)
heatmap.2.mod(DistCor[1:1024,1:1024],
dendrogram = "none",scale="none",trace = "none",Rowv=NULL,Colv = NULL,
xlab="Samples",ylab="Samples",
cexRow = 0.01,cexCol=0.01,cex.lab=5,
key.xlab = "Distance",key.title = "",
key.par = list(cex.main=0.01, cex.lab=2.5,cex.axis=2,
mar=c(5,4.5,4,2)+0.1,mgp=c(3,1,0)),
#usr=c(-2,20,0,1)),xaxp=c(0,15,3),yaxp=c(0,0.15,3))ylog=TRUE,mgp=c(3,0.75,0),
keysize=0.66,
mgp=c(3,0.5,0),margins=c(5,5))
dev.off()
}
# Compute ANOSIM, MRPP or ADONIS2 --------------------
anosimSumm=matrix(NA,nrow=levels,ncol=1)
anosimSig=matrix(NA,nrow=levels,ncol=1) # Will not be printed, since there is plotCtrl below for that
for(i in 1:levels){ # for each level
if(setMethod=="anosim"){
AA=anosim(DistCor.dist, as.factor(partMat[,i]), permutations = Nperm0)
anosimSumm[i]=AA$statistic
anosimSig[i]=AA$signif
}else if(setMethod=="mrpp"){
AA=mrpp(DistCor.dist, as.factor(partMat[,i]), permutations = Nperm0)
anosimSumm[i]=AA$delta
anosimSig[i]=AA$Pvalue
}else{
tmp.frame=as.data.frame(partMat[,i])
colnames(tmp.frame)="X"
AA=adonis2(DistCor.dist~X , tmp.frame, permutations = Nperm0)
anosimSumm[i]=AA$F[1]
anosimSig[i]=AA$`Pr(>F)`[1]
}
}
# ... Plot for the observed anosim
if(setMethod == "anosim"){
methodLab="ANOSIM"
ylabel=paste(methodLab," - R",sep="")
}else if(setMethod=="mrpp"){
methodLab="MRPP"
ylabel=paste(methodLab," - delta",sep="")
}else{
methodLab="ADONIS"
ylabel=paste(methodLab," - F",sep="")
}
df=data.frame(c(1:levels),DistMeans[1:levels],anosimSumm)
colnames(df)=c("Levels","Distance","Summary")
print(ggplot(df, aes(Distance,Summary))+
geom_point()+geom_line()+
ggtitle(paste("Stdv = ",stdv))+
ylab(ylabel) +xlab("Distance Mean")+
scale_size_manual(values=12)+
theme_bw() +
theme(axis.title = element_text(size = 25),
axis.text = element_text(size = 18),
legend.title = element_text(size=18),
legend.text=element_text(size=16),
legend.key.size = unit(.5,"cm")))
print(ggplot(df, aes(Levels,Summary))+
geom_point()+geom_line()+
ggtitle(paste("Stdv = ",stdv))+
ylab(ylabel) +xlab("Levels")+
scale_size_manual(values=12)+
theme_bw() +
theme(axis.title = element_text(size = 25),
axis.text = element_text(size = 18),
legend.title = element_text(size=18),
legend.text=element_text(size=16),
legend.key.size = unit(.5,"cm")))
heatmap.2.mod(DistCor[1:1024,1:1024],
dendrogram = "none",scale="none",trace = "none",Rowv=NULL,Colv = NULL,
xlab="Samples",ylab="Samples",
cexRow = 0.01,cexCol=0.01,cex.lab=5,
key.xlab = "Distance",key.title = "",
key.par = list(cex.main=0.01, cex.lab=2.5,cex.axis=2,
mar=c(5,4.5,4,2)+0.1,mgp=c(3,1,0)),
#usr=c(-2,20,0,1)),xaxp=c(0,15,3),yaxp=c(0,0.15,3))ylog=TRUE,mgp=c(3,0.75,0),
keysize=0.66,
mgp=c(3,0.5,0),margins=c(5,5))
source("/home/apascual/Dropbox/alberto.pascual/Dropbox/Research/Programs/libraries/R/heatmap.2.mod.R")
heatmap.2.mod(DistCor[1:1024,1:1024],
dendrogram = "none",scale="none",trace = "none",Rowv=NULL,Colv = NULL,
xlab="Samples",ylab="Samples",
cexRow = 0.01,cexCol=0.01,cex.lab=5,
key.xlab = "Distance",key.title = "",
key.par = list(cex.main=0.01, cex.lab=2.5,cex.axis=2,
mar=c(5,4.5,4,2)+0.1,mgp=c(3,1,0)),
#usr=c(-2,20,0,1)),xaxp=c(0,15,3),yaxp=c(0,0.15,3))ylog=TRUE,mgp=c(3,0.75,0),
keysize=0.66,
mgp=c(3,0.5,0),margins=c(5,5))
heatmap.2.mod(DistCor[1:1024,1:1024],
dendrogram = "none",scale="none",trace = "none",Rowv=NULL,Colv = NULL,
xlab="Samples",ylab="Samples",
cexRow = 0.01,cexCol=0.01,cex.lab=5,
key.xlab = "Distance",key.title = "",
key.par = list(cex.main=0.01, cex.lab=2.5,cex.axis=2,
mar=c(5,4.5,4,2)+0.1,mgp=c(3,1,0)),
#usr=c(-2,20,0,1)),xaxp=c(0,15,3),yaxp=c(0,0.15,3))ylog=TRUE,mgp=c(3,0.75,0),
keysize=0.66,
mgp=c(3,0.5,0),margins=c(5,5))
AA
i
i=1
tmp.frame=as.data.frame(partMat[,i])
colnames(tmp.frame)="X"
AA=adonis2(DistCor.dist~X , tmp.frame, permutations = Nperm0)
AA
View(partMat)
#-- Load packages needed
library(gplots)
library(MASS)
library(lavaan) #version 0.63 #
library(plyr) # To use join_all
### START EDITING
# Set the working directory (path for the repo)
dirRoot="/home/apascual/Nextcloud/Research/Projects/FunctionalGroups/Repositories/TreeHoles_descriptive/"
setwd(dirRoot)
# --- Parameters for the model:
rndMod=1 # If you want to run a model with the classes randomized fix to one
group=1 # Are you considering the different classes? if not=0
# --- Parameters for the model:
rndMod=0 # If you want to run a model with the classes randomized fix to one
# ..... Fix the path and file
pathModel="data/LavaanModels/"
selectModel="Manifest_Milestone3.lav"
#--- Fix here the paths of input and output files
# .... The functions
pathFun="data/source/"
fileFun="20151016_Functions_remainder.csv"
# .... The properties of the functional groups you will use for your model
pathClass="data/source/"
fileClass="Partition_PamClustering_SamplesTime0_SparCC.tsv.vec"
# .... The output path (The files saved with the model are created according with "selectModel" variable)
pathOut="data/SEM"
# Read Files --------------------------------------------------------------
#-- Read the clusters to which each sample is associated
setwd(pathClass)
metadata=read.table(fileClass)
View(metadata)
fileClass="samples_metadata_time0.tsv"
metadata=read.table(fileClass,sep="\t")
Class=metadata[,c(1,4)] # samples and SparCC partition
# ..... create a randomization of class
idxx=permute::shuffle(dim(Class)[1])
ClassRnd=Class[idxx,2]
Class=cbind(Class,ClassRnd)
colnames(Class)=c("Community","Id","Rnd")
#-- Read in functional data, and perform some operations
setwd(pathFun)
#-- Read in functional data, and perform some operations
dd.data=read.csv(fileFun)
getwd()
fileFun="20151016_Functions_remainder.clean.csv"
#-- Read in functional data, and perform some operations
dd.data=read.csv(fileFun)
#---- Exclude negative controls
dd.data=dd.data[dd.data$Community!="blank",]
dd.data=na.omit(dd.data)
#---- Take just  functions at time 7 and exclude normalized respiration
idxx=colnames(dd.data)
dd.data=subset(dd.data,select=c(Community,Replicate,grep("7", idxx)))
idxx
#-- Read in functional data, and perform some operations
dd.data=read.csv(fileFun)
View(dd.data)
#-- Read in functional data, and perform some operations
dd.data=read.csv(fileFun,sep="\t")
#-- Read in functional data, and perform some operations
dd.data=read.csv(fileFun,sep=",")
fileFun="20151016_Functions_remainder.csv"
#-- Read in functional data, and perform some operations
dd.data=read.csv(fileFun)
#---- Exclude negative controls
dd.data=dd.data[dd.data$Community!="blank",]
dd.data=na.omit(dd.data)
#---- Take just  functions at time 7 and exclude normalized respiration
idxx=colnames(dd.data)
dd.data=subset(dd.data,select=c(Community,Replicate,grep("7", idxx)))
dd.data=subset(dd.data,select= -c(pgRPC.7))
# Average and measurement error across replicas --------------------
#---- Average function measurements across the replicates
dd=aggregate(dd.data[3:ncol(dd.data)],list(dd.data$Community),function(x){mean(x,na.rm=T)})
row.names(dd)=unique(dd.data$Community)
colnames(dd)[1]="Community"
#---- Change the units
dd[,2:ncol(dd)]=log(dd[,2:ncol(dd)]+1) # Pseudocount
#---- Split function measurements across the replicates and look for common samples
dd.split=split(dd.data,dd.data$Replicate) # We first create a list split by replica
dd.join=join_all(dd.split,by="Community") # The problem is that not all the replicas have all samples, so first join them with a common community col
dd.join.clean=dd.join[complete.cases(dd.join),] # so we get just complete rows
idx.consens=dd.join.clean$Community # and now the names
Nrep=4
dd.split.log=list()
for(i in 1:Nrep){
matched=match(idx.consens,dd.split[[i]]$Community)
dd.split[[i]]=dd.split[[i]][matched,]
#rownames(dd.split[[i]])=unique(dd.split[[i]]$Community)
dd.split[[i]]=subset(dd.split[[i]],select=-c(Community,Replicate))
dd.split.log[[i]]=log(dd.split[[i]]+1)
}
# ---- Compute measurement error (average of the all-against-all correlations)
Ncols=dim(dd.split.log[[1]])[2]
Npair=Nrep*(Nrep-1)/2
meas.err=matrix()
for(k in 1:Ncols){
corr=0
for(i in 1:(Nrep-1)){
for(j in (i+1):Nrep){
corr=corr+cor(dd.split.log[[i]][,k],dd.split.log[[j]][,k],use="complete.obs")
}
}
corr=corr/Npair
meas.err[k]=corr
}
#---- Build a new dataset with class
dd.all=join_all(list(dd,Class),by="Community")
dd.all=dd.all[complete.cases(dd.all),]
dd.all$Id=factor(dd.all$Id)
# --- Build dummy columns from partition identities and an additional one with random identities
Id.f = dd.all$Id
dummies = model.matrix(~0+Id.f)
colnames(dummies)[1]="Id.f1"
Id.f.rnd= as.data.frame(sample(dd.all$Id)) # Generate random identities
colnames(Id.f.rnd)[1]="Id.rnd"
dd.all=cbind(dd.all,Id.f.rnd,dummies)
# Compute SEM --------------------------------
myModel <- readLines(paste(pathModel,selectModel,sep=""))
setwd(dirRoot)
# Compute SEM --------------------------------
setwd(pathModel)
myModel <- readLines(paste(pathModel,selectModel,sep=""))
getwd()
myModel <- readLines(selectModel)
setwd(dirRoot)
# ..... fit model
if(group==0){
fit <- sem(myModel, data=dd.all)
}else{
if(rndMod==0){ # Run a regular model
selectModel=paste(selectModel,"group",sep=".") # Rename the label for the output files
fit <- sem(myModel, data=dd.all, group="Id")
}else{ # Run a randomization
selectModel=paste(selectModel,"group","Rnd2",sep=".") # Rename the label for the output files
fit <- sem(myModel, data=dd.all, group="Rnd")
}
}
summary(fit, standardized=T,rsq=T, fit.measures = TRUE)
source('~/APASCUAL/Research/Programs/microbacteria/FunctionalGroups/SEM_2pub.R')
source('~/APASCUAL/Research/Programs/microbacteria/FunctionalGroups/SEM_2pub.R')
source('~/APASCUAL/Research/Programs/microbacteria/FunctionalGroups/SEM_2pub.R')
source('~/Nextcloud/Research/Projects/FunctionalGroups/Repositories/TreeHoles_descriptive/scripts/LocationRandHierarchy2method_V2.R')
warnings()
source('~/Nextcloud/Research/Projects/FunctionalGroups/Repositories/TreeHoles_descriptive/scripts/LocationRandHierarchy2syntheticData.R')
